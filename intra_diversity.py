import os, json, random
from typing import Iterable

import numpy as np
from functools import partial
from multiprocessing.pool import Pool
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
from tqdm import tqdm

model_list = [
    'EleutherAI/pythia-2.8b',
    'ContextualAI/archangel_sft_pythia2-8b',
    'ContextualAI/archangel_sft-ppo_pythia2-8b',
    'ContextualAI/archangel_sft-dpo_pythia2-8b',
]

result_dir = 'model_outputs'
dataset_name = 'euclaise/writingprompts'
dataset_split = 'validation'
num_samples = 1000

# A generation is a sequence of rows, where each row is a list of sentences
# Sentences on the same row are generated by the same prompt
def distinctness(generations: Iterable[list[str]]):
    dist1, dist2, dist3 = [], [], []

    for row in generations:
        unigrams, bigrams, trigrams = set(), set(), set()
        total_words = 0
        for sentence in row:
            o = sentence.split()
            # o = [str(tok) for tok in gen]
            total_words += len(o)
            unigrams.update(o)
            for i in range(len(o) - 1):
                bigrams.add(o[i] + '_' + o[i+1])
            for i in range(len(o) - 2):
                trigrams.add(o[i] + '_' + o[i+1] + '_' + o[i+2])
        if total_words == 0:
            continue
        dist1.append(len(unigrams) / total_words)
        dist2.append(len(bigrams) / total_words)
        dist3.append(len(trigrams) / total_words)

    return np.mean(dist1), np.mean(dist2), np.mean(dist3)

def bleu_i(weights, all_sentences, smoothing_function, i):
    # noinspection PyTypeChecker
    return np.array(sentence_bleu(
        references=all_sentences[:i] + all_sentences[i + 1:],
        hypothesis=all_sentences[i],
        weights=weights,
        smoothing_function=smoothing_function))

def self_bleu(generations: Iterable[list[str]], n_sample=1000):
    random.seed(0)

    smoothing_function = SmoothingFunction().method1
    all_sentences = []
    for row in generations:
        gens = [sentence.split() for sentence in row]
        all_sentences += gens
    n_sample = min(n_sample, len(all_sentences))

    pool = Pool(processes=os.cpu_count())
    weights = [
        (1.0, 0, 0, 0),
        (0.5, 0.5, 0, 0),
        (1.0 / 3, 1.0 / 3, 1.0 / 3, 0),
        (0.25, 0.25, 0.25, 0.25),
        (0.2, 0.2, 0.2, 0.2, 0.2)]

    bleus = sum(tqdm(
        pool.imap_unordered(
            partial(bleu_i, weights, all_sentences, smoothing_function),
            random.sample(range(len(all_sentences)), n_sample)),
        total=n_sample,
        smoothing=0.0,
        desc=f"bleu")) / n_sample

    pool.close()
    pool.join()

    return bleus


for model_name_or_path in model_list:
    result_file = f"{result_dir}/{model_name_or_path.replace('/', '_')}_{dataset_name.replace('/', '_')}_{dataset_split}_generations_{num_samples}.json"
    with open(result_file) as f:
        results: list[dict[str,str]] = json.load(f)
    assert len(results) == num_samples

    generations = list(map(lambda r: [r['generated']], results))
    dist1, dist2, dist3 = distinctness(generations)
    blues1, blues2, blues3, blues4, blues5 = self_bleu(generations)

    print(f'# {model_name_or_path}')
    print(f'dist1 = {dist1:.4f}')
    print(f'dist2 = {dist2:.4f}')
    print(f'dist3 = {dist3:.4f}')
    print(f'blues1 = {blues1:.4f}')
    print(f'blues2 = {blues2:.4f}')
    print(f'blues3 = {blues3:.4f}')
    print(f'blues4 = {blues4:.4f}')
    print(f'blues5 = {blues5:.4f}')
