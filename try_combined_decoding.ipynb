{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c3d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /scratch/yl13579/.conda/envs/trl/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /scratch/yl13579/.conda/envs/trl/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /scratch/yl13579/.conda/envs/trl/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1ec26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity_gen.py    model_outputs     try_combined_decoding.ipynb\n",
      "diversity.ipynb     __pycache__       try_load_model_logits.ipynb\n",
      "intra_diversity.py  README.md\t      utils.py\n",
      "logit_lens.ipynb    requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ca8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intra_diversity import *\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61eba92e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "post_samples_path = './model_outputs/ContextualAI_archangel_sft_pythia2-8b_euclaise_writingprompts_validation_samples_100.pkl'\n",
    "base_samples_path = './model_outputs/EleutherAI_pythia-2.8b_euclaise_writingprompts_validation_samples_100.pkl'\n",
    "#here the samples are generated in a teacher forcing manner, so the model is not generating the text, but rather the text is being generated by a teacher model\n",
    "\n",
    "with open(post_samples_path, 'rb') as f:\n",
    "    post_samples = pickle.load(f)\n",
    "with open(base_samples_path, 'rb') as f:\n",
    "    base_samples = pickle.load(f)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-2.8b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea649cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_top_k_for_one_token(sample_a, sample_b,  token_pos, k=10, sample_a_name='Sample A', sample_b_name='Sample B'):\n",
    "    \"\"\"\n",
    "    Compare the top k token prob distribution of two samples.\n",
    "    Note: The distribution plot might have [k, 2k] tokens, \n",
    "          these are combined top-k logits from both samples.\n",
    "    \"\"\"\n",
    "    probs_a = sample_a['generated_logits'].softmax(dim=-1)[token_pos].unsqueeze(0)\n",
    "    probs_b = sample_b['generated_logits'].softmax(dim=-1)[token_pos].unsqueeze(0)\n",
    "\n",
    "    top_k_a = probs_a.topk(k, dim=-1).indices\n",
    "    top_k_b = probs_b.topk(k, dim=-1).indices\n",
    "\n",
    "    # combine to get all the token ids (unique)\n",
    "    combined_top_k = torch.unique(torch.cat([top_k_a, top_k_b], dim=-1)).tolist()\n",
    "    combined_probs_a = probs_a[0, combined_top_k].numpy()\n",
    "    combined_probs_b = probs_b[0, combined_top_k].numpy()\n",
    "    \n",
    "    #measure sharpness by calc the cumulative sum of the probs\n",
    "    combined_probs_a_cumsum = np.cumsum(combined_probs_a)\n",
    "    combined_probs_b_cumsum = np.cumsum(combined_probs_b)\n",
    "    \n",
    "    #a distribution is sharp if the top 3 cumsum >= 0.9\n",
    "    a_is_sharp = combined_probs_a_cumsum[2] >= 0.9\n",
    "    b_is_sharp = combined_probs_b_cumsum[2] >= 0.9\n",
    "    \n",
    "    if a_is_sharp and b_is_sharp:\n",
    "        print(f\"Both {sample_a_name} and {sample_b_name} are sharp.\")\n",
    "        #here we will sample from a (SFT version)\n",
    "        next_token = random.choices(top_k_a) #[token_id]\n",
    "    elif a_is_sharp and not b_is_sharp:\n",
    "        print(f\"{sample_a_name} is sharp, but {sample_b_name} is not.\")\n",
    "        #here we will sample from combined top k\n",
    "        next_token = random.choices(combined_top_k)\n",
    "    else:\n",
    "        #here we sample from a (SFT version)\n",
    "        print(f\"{sample_a_name} is not sharp, but {sample_b_name} is.\")\n",
    "        next_token = random.choices(top_k_a)\n",
    "    return next_token[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_top_k(sample,token_pos, k=10):\n",
    "    \"\"\"\n",
    "    Get the top k token prob distribution of a sample.\n",
    "    \"\"\"\n",
    "    probs = sample['generated_logits'].softmax(dim=-1)[token_pos].unsqueeze(0)\n",
    "    top_k = probs.topk(k, dim=-1).indices\n",
    "    return random.choices(top_k)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e45141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diverse_one_token(sample_sft,sample_base,token_pos,tokenizer):\n",
    "    \"\"\"\n",
    "    for one dp,  given teacher-forcing logits sample_sft and sample_base,\n",
    "    sample the next token with diversity\n",
    "    \"\"\"\n",
    "    story_prefix_ids = tokenizer(sample_sft[\"story\"]).input_ids[:token_pos]\n",
    "    story_prefix = tokenizer.decode(story_prefix_ids)\n",
    "    \n",
    "    next_token = get_combined_top_k_for_one_token(sample_sft, sample_base,  token_pos, k=10, sample_a_name='SFT', sample_b_name='Base')\n",
    "    next_token_sft = get_single_top_k(sample_sft, token_pos, k=10)\n",
    "    next_token_base = get_single_top_k(sample_base, token_pos, k=10)\n",
    "    \n",
    "    combined_seq = story_prefix + tokenizer.decode(next_token)\n",
    "    combined_seq_sft = story_prefix + tokenizer.decode(next_token_sft)\n",
    "    combined_seq_base = story_prefix + tokenizer.decode(next_token_base)\n",
    "    \n",
    "    #print table: sample method, prefix, next token\n",
    "    print(f\"{'sample method':<20} {'prefix':<50} {'next token':<20}\")\n",
    "    print(f\"{'combined':<20} {story_prefix:<50} {tokenizer.decode(next_token):<20}\")\n",
    "    print(f\"{'sft':<20} {story_prefix:<50} {tokenizer.decode(next_token_sft):<20}\")\n",
    "    print(f\"{'base':<20} {story_prefix:<50} {tokenizer.decode(next_token_base):<20}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
